<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-16T08:25:17+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">msaidov.com</title><subtitle></subtitle><author><name>Marat Saidov</name><email>dev@msaidov.com</email></author><entry><title type="html">Phi Silica Small But Mighty Small Language Model</title><link href="http://localhost:4000/phisilica/" rel="alternate" type="text/html" title="Phi Silica Small But Mighty Small Language Model" /><published>2024-12-22T00:00:00+01:00</published><updated>2024-12-22T00:00:00+01:00</updated><id>http://localhost:4000/phi-silica-small-but-mighty-small-language-model</id><content type="html" xml:base="http://localhost:4000/phisilica/"><![CDATA[<p>A technical blog about Phi Silica – Microsoft’s on-device language model.</p>

<p>The blog covers PTQ, GQA, KV caches, and strategies to increase context length under the constraints of NPU resources (since other models, in addition to Phi Silica, also need to fit on the chip).</p>

<p>I also managed to contribute, albeit a little: <a href="https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/">Windows Blogs</a>.</p>]]></content><author><name>Marat Saidov</name><email>dev@msaidov.com</email></author><summary type="html"><![CDATA[A technical blog about Phi Silica – Microsoft’s on-device language model.]]></summary></entry><entry><title type="html">Corpus Of Artificial Texts</title><link href="http://localhost:4000/coat/" rel="alternate" type="text/html" title="Corpus Of Artificial Texts" /><published>2024-09-23T00:00:00+02:00</published><updated>2024-09-23T00:00:00+02:00</updated><id>http://localhost:4000/corpus-of-artificial-texts</id><content type="html" xml:base="http://localhost:4000/coat/"><![CDATA[<p>CoAT: Corpus of Artificial Texts.</p>

<p>I am happy to share <a href="https://www.cambridge.org/core/journals/natural-language-processing/article/coat-corpus-of-artificial-texts/7E2CA97E21663CC031FB6BAFE56E0046">my most recent paper</a>. The work is done during my research at <a href="https://cs.hse.ru/en/ai/computational-pragmatics/">MMCP Lab, HSE University</a>.</p>

<p>CoAT is a large-scale corpus of human-written and AI-generated texts in Russian, spanning 6 domains and 13 different text generation models. The study provides insights into artificial text detection capabilities and challenges, highlighting the need for improved detection methods as language models continue to advance.</p>]]></content><author><name>Marat Saidov</name><email>dev@msaidov.com</email></author><summary type="html"><![CDATA[CoAT: Corpus of Artificial Texts.]]></summary></entry><entry><title type="html">Asian Journey In 2022</title><link href="http://localhost:4000/asian-journey/" rel="alternate" type="text/html" title="Asian Journey In 2022" /><published>2023-02-10T00:00:00+01:00</published><updated>2023-02-10T00:00:00+01:00</updated><id>http://localhost:4000/asian-journey-in-2022</id><content type="html" xml:base="http://localhost:4000/asian-journey/"><![CDATA[<p>I am fond of the Western attitude towards a gap year.</p>

<p>There exist travel programs for those who have obtained a BSc and wish to take a break before pursuing their MSc. <br />
In my case, the gap year turned out to be involuntary. Furthermore, it extended far beyond a year. Nevertheless, it harmoniously aligned with the Western approach.</p>

<p>I left Russia at the end of September and only found a new home in February. Although I have yet to tackle lengthy posts about each of the countries I had the opportunity to visit, I desire to showcase the route and document this extraordinary journey.</p>

<figure class=" ">
  
    
      <a href="/assets/images/asian-journey-overview.png" title="Route.">
          <img src="/assets/images/asian-journey-overview.png" alt="" />
      </a>
    
  
  
</figure>

<p>Kazakhstan 🇰🇿 -&gt; India 🇮🇳 -&gt; Thailand 🇹🇭 -&gt; Malaysia 🇲🇾 -&gt; Thailand Again 🇹🇭 -&gt; Vietnam 🇻🇳… and finally, Serbia 🇷🇸.</p>

<p>The power of such adventures lies in the everyday challenges. New challenges. Ones that had never arisen before. For instance, how does one mount a scooter if they have never even sat on a bicycle? How does one persuade stern Vietnamese border officials at Saigon airport that I am flying to Serbia not as a tourist, but for a new job awaiting me there?</p>

<p>It was truly fascinating. I will never replicate anything similar.</p>]]></content><author><name>Marat Saidov</name><email>dev@msaidov.com</email></author><summary type="html"><![CDATA[I am fond of the Western attitude towards a gap year.]]></summary></entry><entry><title type="html">The Rps Race</title><link href="http://localhost:4000/the-rps-race/" rel="alternate" type="text/html" title="The Rps Race" /><published>2022-11-14T00:00:00+01:00</published><updated>2022-11-14T00:00:00+01:00</updated><id>http://localhost:4000/the-RPS-race</id><content type="html" xml:base="http://localhost:4000/the-rps-race/"><![CDATA[<p>The MVP for a model serving cloud-native solution.</p>

<p>At my previous role, we actively set up a production environment for deploying hundreds of machine learning models. I want to talk about the central metric in this task - RPS (requests per second) - and the first step taken to improve it.</p>

<p>Typically, RPS is associated with Site Reliability Engineering. Often, it involves web services with heavy backends, sprawling databases, and a large pool of users. For example, the SRE team at Google recently exceeded 3,500 people. <a href="https://sre.google/">Ben Treynor, VP of SRE at Google, explains</a>: “Typically, we hire about a 50-50 mix of people who have more of a software background and people who have more of a systems engineering background. It seems to be a really good mix.”</p>

<p>In the field of machine learning, I would highlight some preliminary knowledge required for developing such an ML environment:</p>

<ul>
  <li>DevOps and infrastructure development;</li>
  <li>Building end-to-end ML pipelines: from data preprocessing and feature selection to parameter tuning;</li>
  <li>Understanding the open-source world of model deployment and how to leverage it to avoid reinventing the wheel.</li>
</ul>

<p>Ultimately, it becomes a mix of hardcore engineering, non-trivial analytics, and system design. It sounds inspiring until you start measuring the system’s performance.</p>

<p>🐌 RPS = 21</p>

<p>In its most basic version, we achieved 21 requests per second, or a minute-long wait for a response under a load of around 1200 users. Unacceptable. We need to identify the bottleneck.</p>

<p>In the image below, the folks at MLflow propose a model storage architecture. AWS S3 is used for storing the weights. However, data transfer occurs over the network (a.k.a. slowly).</p>

<figure class=" ">
  
    
      <a href="/assets/images/mlflow.jpg" title="MLflow model registry including S3 bucket.">
          <img src="/assets/images/mlflow.jpg" alt="" />
      </a>
    
  
  
    <figcaption>Source: https://mlflow.org/docs/latest/model-registry.html
</figcaption>
  
</figure>

<p>It was noticed that the model weights were being transmitted over the network with every request to the environment, which was a severe bottleneck. Let’s keep the models in RAM after the initial transfer.</p>

<p>🏃‍♂️ RPS = 155</p>

<p>A simple heuristic, yet the acceleration is more than sevenfold. In the end, through a trade-off between memory and speed, we achieved acceptable system performance for the first version of the product. This means we can proudly call ourselves MLOps engineers.</p>]]></content><author><name>Marat Saidov</name><email>dev@msaidov.com</email></author><summary type="html"><![CDATA[The MVP for a model serving cloud-native solution.]]></summary></entry></feed>